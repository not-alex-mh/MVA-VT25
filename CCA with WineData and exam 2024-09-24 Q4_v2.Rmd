---
title: "Canonical Correlations by Hand and with Package"
author: "Alexander M. Holmsk√§r"
date: "2025-03-19"
output:
  pdf_document: default
  html_document: default
subtitle: "Including MvA exam 2024-09-24 Exercise 4"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(expm)
library(CCA)
library(CCP)
```

# CCA Package vs By Hand

The first section cross checks the results we would get if we had raw data instead of just a covariance matrix. I use the WineData from a previous homework.
We rebuild the matrix so that the smaller set of variables are in the upper left corner. This is why we get X1  and X2. By convention, $p \leq q$.

```{r}
wine <- read.table("WineData.txt", header=TRUE)
wine1 <- as.matrix(wine)

X2 <- cbind(wine1[,3:5])
X1 <- cbind(wine1[,1:2])
```

Get the correlation matrix by 

$$
\mathbf{D}^{-1/2}\mathbf{S}\mathbf{D}^{-1/2}
$$
where D is the diagonal of the square matrix.

```{r}
cov_w <- cov(cbind(X1, X2)) # _w for wine
cor_w <- solve(expm::sqrtm(diag(diag(cov_w)))) %*% cov_w %*% solve(expm::sqrtm(diag(diag(cov_w))))
cor_w
```
Compare the manual calculation with R.
```{r}
wine2 <- cbind(X1, X2)
cor(wine2)
```

Next, partition the matrix and calculate
$$
\mathbf{R}^{-1/2}_{11} \mathbf{R}_{12} \mathbf{R}^{-1}_{22} \mathbf{R}_{21} \mathbf{R}^{-1/2}_{11} \\
\mathbf{R}^{-1/2}_{22} \mathbf{R}_{21} \mathbf{R}^{-1}_{11} \mathbf{R}_{12} \mathbf{R}^{-1/2}_{22}
$$
to get the matrices that will give us our $\lambda$'s and $\rho$'s.
The square root of the $\lambda$'s  corresponding to the first equation above, will yield our canonical correlations $\rho_{i}$. In the code below we will let rrRrr_e correspond to the first equation above, and rrRrr_f to the second. From these, we will then get our *e* and *f* eigenvectors.

```{r}
r11 <- cor_w[1:2, 1:2]
r12 <- cor_w[1:2, 3:5]
r21 <- cor_w[3:5, 1:2]
r22 <- cor_w[3:5, 3:5]

rrRrr_e <- solve(sqrtm(r11)) %*% r12 %*% solve(r22) %*% r21 %*% solve(sqrtm(r11))
rrRrr_f <- solve(sqrtm(r22)) %*% r21 %*% solve(r11) %*% r12 %*% solve(sqrtm(r22))

eigens_e <- eigen(rrRrr_e)
eigens_f <- eigen(rrRrr_f)

canonical_rho <- sqrt(eigens_e$values)
canonical_rho
```

Cross check with CCA to make sure we are not retarded.
```{r}
cc1 <- cc(X1, X2)
cc1$cor
# Wohoo
```

To get the coefficients *a* we use equation 10-8 from page 541 in the book.
$$
U_{k} = \mathbf{a}^{\top}_{k} \mathbf{Z}^{(1)} = \mathbf{e}^{\top}_{k} \mathbf{R}^{-1/2}_{11} \mathbf{Z}^{(1)}\\
$$
implying
$$
\mathbf{a}^{\top}_{k} = \mathbf{e}^{\top} \mathbf{R}^{-1/2}_{11} \\
\mathbf{a}_{k} = \left( \mathbf{e}^{\top} \mathbf{R}^{-1/2}_{11} \right)^{\top}.
$$
Get all $a_{k}$ and $b_{k}$. Note *A* and *B* will have rows that contain the coefficients for X1 and X2 respectively
```{r}
a1_t <- t(eigens_e$vectors[,1]) %*% solve(sqrtm(r11)) # For U1
a2_t <- t(eigens_e$vectors[,2]) %*% solve(sqrtm(r11)) # For U2
A <- rbind(a1_t, a2_t)
A

b1_t <- t(eigens_f$vectors[,1]) %*% solve(sqrtm(r22))
b2_t <- t(eigens_f$vectors[,2]) %*% solve(sqrtm(r22))
b3_t <- t(eigens_f$vectors[,3]) %*% solve(sqrtm(r22))
B <- rbind(b1_t, b2_t, b3_t)
B
```
Note that we can also get the correct *A* and *B* matrices straight away if we multiply the matrices
$$
\mathbf{A} = \mathbf{E}^{\top} \mathbf{R}^{-1/2}_{11} \\
\mathbf{B} = \mathbf{F}^{\top} \mathbf{R}^{-1/2}_{22}
$$
where *E* is the matrix containing are all the eigenvectors from rrRrr_e and *F* for rrRrr_f.
```{r}
t(eigens_e$vectors) %*% solve(sqrtm(r11))
t(eigens_f$vectors) %*% solve(sqrtm(r22))
```

Compare raw and standardized coefficients from CCA. Get the standardized coefficients using a diagonal matrix containing the standard deviations of the original sets of variables. Pay extra attention to how the CCA package reports xcoef in a transposed format, compared to the book calculations. If we transpose our *A*, we get the same matrix as CCA, which is NOT what we want, if we want to follow the book formulas.
```{r}
# Raw:
# cc1$xcoef
# Standardiced:
s11 <- cov_w[1:2, 1:2]
s22 <- cov_w[3:5, 3:5]
sqrtm(diag(diag(s11))) %*% cc1$xcoef
t(A)
diag(sqrt(diag(s22))) %*% cc1$ycoef
```

The orientation of our *A* will have consequences for when we compare the correlations between U and X1. The manual calculations for R_U_X1 will follow the convention, where a row is the canonical variates. Contrast to CCA, where the columns are the canonical covariates. The CCA package reports $\mathbf{A}^{-1}$, where columns correspond to canonical variates.
Use the first two equations of 10-19 on page 55.

$$
\mathbf{R}_{\hat{\mathbf{U}}, \mathbf{x}^{(1)}} = \hat{\mathbf{A}} \mathbf{S}_{11} \mathbf{D}_{11}^{-1/2} \\
\mathbf{R}_{\hat{\mathbf{V}}, \mathbf{x}^{(2)}} = \hat{\mathbf{B}} \mathbf{S}_{22} \mathbf{D}_{22}^{-1/2}
$$
In our case we have standardized our variables, so 
$$
\mathbf{S}_{11} \mathbf{D}^{-1/2}_{11}=\mathbf{R}_{11}
$$

```{r}
R_U_X1 <- A %*% r11
rownames(R_U_X1) <- c("U1", "U2")
R_U_X1
cc1$scores$corr.X.xscores
```

Note that we get an extra V when we calculate by hand, since X2 contain 3 variables. The third canonical variate V3 is irrelevant because there is no third U3 to correlate it to
```{r}
R_V_X2 <- B %*% r22
rownames(R_V_X2) <- c("V1", "V2", "V3")
R_V_X2
cc1$scores$corr.Y.yscores
```

Variance explained by hand and using CCA and the book.
```{r}
rowMeans(R_U_X1^2)
colMeans(cc1$scores$corr.X.xscores^2)

# In the labs we used the code below, which is irrelevant since those numberd are stored in the cc() output anyway
#cc2 <- comput(X1, X2, cc1)
#colMeans((cc2$corr.X.xscores)^2)

rowMeans(R_V_X2^2)
colMeans(cc1$scores$corr.Y.yscores^2)
```
It feels counterintuitive that the second canonical variates U2 and V2 should explain more of their respecitive sample variances. But I think this is an irrelevant consequence. The correlation between U1 and V1 is maximized, not the correlation between U1 and X1 nor V1 and X2.
We can get the same numbers using the equations on page 562.
$$
\text{tr}(\hat{\mathbf{a}}_{\mathbf{z}}^{(1)} \hat{\mathbf{a}}_{\mathbf{z}}^{(1)\top} + \hat{\mathbf{a}}_{\mathbf{z}}^{(2)} \hat{\mathbf{a}}_{\mathbf{z}}^{(2)\top} + \dots + \hat{\mathbf{a}}_{\mathbf{z}}^{(r)} \hat{\mathbf{a}}_{\mathbf{z}}^{(r)\top}) =
\sum^{r}_{i=1} \sum^{p}_{k=1} r^{2}_{\hat{U}_{i}, z^{(1)}_{k}}
$$
Get the proportion of variance explained by the first U by dividing the double sum by the number of variables p.
```{r}
A_inv <- solve(A)
a1_hat <- A_inv[,1]
a2_hat <- A_inv[,2]
```

Proportion of total sample variance explained by U1
```{r}
sum(diag(t(a1_hat) %*% a1_hat )) /2
```
Proportion of total sample variance explained by U2
```{r}
sum(diag(t(a2_hat) %*% a2_hat )) /2
```

And for V1 and V2
```{r}
B_inv <- solve(B)
b1_hat <- B_inv[,1]
b2_hat <- B_inv[,2]
sum(diag(t(b1_hat) %*% b1_hat )) /3
sum(diag(t(b2_hat) %*% b2_hat )) /3
```

Maybe we want to see how much variance U1 explains in set X2. Then we use equations 10-29 on page 552 again.
$$
\mathbf{R}_{\hat{\mathbf{U}}, \mathbf{x}^{(2)}} = \hat{\mathbf{A}} \mathbf{S}_{12} \mathbf{D}_{22}^{-1/2} \\
\mathbf{R}_{\hat{\mathbf{V}}, \mathbf{x}^{(1)}} = \hat{\mathbf{B}} \mathbf{S}_{21} \mathbf{D}_{11}^{-1/2}
$$
By and and using CCA
```{r}
R_U_X2 <- A %*% r12
rownames(R_U_X2) <- c("U1", "U2")
rowMeans(R_U_X2^2)
colMeans(cc1$scores$corr.Y.xscores^2)
```

```{r}
R_V_X1 <- B %*% r21
rownames(R_V_X1) <- c("V1", "V2", "V3")
round(rowMeans(R_V_X1^2), digits = 6)
colMeans(cc1$scores$corr.X.yscores^2)
```

# 2024-09-24 Exercise 4

Start by standardizing. I name the subsets r_ii after convention, that is the first one will have the fewest variables. Since i use the same variable names, the chunks above will not give the same number if you go back up and run them after running this chunk.
```{r}
S <- matrix(c(
  1106.000,  396.700,  108.400,  0.787,   26.230,
  396.700, 2382.000, 1143.000, -0.214,  -23.960,
  108.400, 1143.000, 2136.000,  2.189,  -20.840,
  0.787,   -0.214,   2.189,   0.016,   0.216,
  26.230,  -23.960, -20.840,   0.216,  70.560
), nrow = 5, byrow = TRUE)

R <- solve(sqrtm(diag(diag(S)))) %*% S %*% solve(sqrtm(diag(diag(S))))

r22 <- R[1:3, 1:3]
r21 <- R[1:3, 4:5]
r12 <- R[4:5, 1:3]
r11 <- R[4:5, 4:5]

rrRrr_e <- solve(sqrtm(r11)) %*% r12 %*% solve(r22) %*% r21 %*% solve(sqrtm(r11))
rrRrr_f <- solve(sqrtm(r22)) %*% r21 %*% solve(r11) %*% r12 %*% solve(sqrtm(r22))

eigens_e <- eigen(rrRrr_e)
eigens_f <- eigen(rrRrr_f)

canonical_rho <- sqrt(eigens_e$values)
canonical_rho
```

```{r}
a1_t <- t(eigens_e$vectors[,1]) %*% solve(sqrtm(r11)) # For U1
a2_t <- t(eigens_e$vectors[,2]) %*% solve(sqrtm(r11)) # For U2
A <- rbind(a1_t, a2_t)
A

b1_t <- t(eigens_f$vectors[,1]) %*% solve(sqrtm(r22))
b2_t <- t(eigens_f$vectors[,2]) %*% solve(sqrtm(r22))
b3_t <- t(eigens_f$vectors[,3]) %*% solve(sqrtm(r22))
B <- rbind(b1_t, b2_t, b3_t)
B
```

```{r}
R_U_X1 <- A %*% r11
rownames(R_U_X1) <- c("U1", "U2")
R_U_X1

R_V_X2 <- B %*% r22
rownames(R_V_X2) <- c("V1", "V2", "V3")
R_V_X2
```
Variance explained.
```{r}
rowMeans(R_U_X1^2)
rowMeans(R_V_X2^2)
```

Alternatively, proportion of total sample variance explained by U1 and U2
```{r}
A_inv <- solve(A)
a1_hat <- A_inv[,1]
a2_hat <- A_inv[,2]
sum(diag(t(a1_hat) %*% a1_hat )) /2
sum(diag(t(a2_hat) %*% a2_hat )) /2
```

And for V1 and V2
```{r}
B_inv <- solve(B)
b1_hat <- B_inv[,1]
b2_hat <- B_inv[,2]
sum(diag(t(b1_hat) %*% b1_hat )) /3
sum(diag(t(b2_hat) %*% b2_hat )) /3
```

Variances explained by U in X2
```{r}
R_U_X2 <- A %*% r12
rownames(R_U_X2) <- c("U1", "U2")
rowMeans(R_U_X2^2)
```
Variances explained by V in X1
```{r}
R_V_X1 <- B %*% r21
rownames(R_V_X1) <- c("V1", "V2", "V3")
round(rowMeans(R_V_X1^2), digits = 6)
```

Next, test the significance using the p.asym function using Wilk's lambda or the Pillai-Bartlett trace.
```{r}
n <- 46
p <- 2
q <- 3
CCP::p.asym(rho = can_cors, N = 46, p = 2, q = 3)
CCP::p.asym(rho = can_cors, N = 46, p = 2, q = 3,
            tstat = "Pillai")
```

```{r}
n * log(det(r11) * det(r22) / det(R))
-n * log((1-can_cors[1]^2) * (1-can_cors[2]^2))
-(n - 1 - 1/2*(p + q + 1)) * log((1-can_cors[1]^2) * (1-can_cors[2]^2))

chi_df1 = p * q
qchisq(p = .05, df = chi_df1, lower.tail=FALSE)

```

I hate canonical correlations.